{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/hiddouche/CNN_NSL_KDD/blob/master/NSL_KDD_with_deep_learning.ipynb\"  target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ihpi-tqsFQfW",
    "outputId": "c50f8515-8de2-4e95-9e70-42335d2caca5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "  from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9qwpnJkEFfFa",
    "outputId": "2b8a9ff6-ae82-4ab9-b347-b115e3d95daf"
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow==1.14.0\n",
    "#!pip install keras==2.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "uB2YUQPvWpy7"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from joblib import dump, load\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score,recall_score\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CC1P_qSlWpy_",
    "outputId": "ba0760c8-69e5-4c32-d392-ae455c204f02"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import keras\n",
    "import sklearn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.layers import LSTM, SimpleRNN, GRU, Bidirectional, BatchNormalization,Convolution1D,MaxPooling1D, Reshape, GlobalAveragePooling1D,Flatten\n",
    "from keras.utils import to_categorical\n",
    "import sklearn.preprocessing\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "from tensorflow.keras.utils import get_file, plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "print(pd.__version__)\n",
    "print(np.__version__)\n",
    "print(sys.version)\n",
    "print(sklearn.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "ZR4fiHTHWpzC",
    "outputId": "2600e4ea-2b2b-457f-bb30-7c0147cbfa54"
   },
   "outputs": [],
   "source": [
    "#Loading training set into dataframe\n",
    "df = pd.read_csv('/content/drive/My Drive/datasets/KDDTrain+.txt', header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "vIVhsN2zWpzF",
    "outputId": "9a4aa122-7fed-4411-e922-49e1a63970c3"
   },
   "outputs": [],
   "source": [
    "#Loading testing set into dataframe\n",
    "qp = pd.read_csv('/content/drive/My Drive/datasets/KDDTest+.txt', header=None)\n",
    "qp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "RLxEUsBjWpzH",
    "outputId": "52a14d94-f7f9-463a-99e6-8b27cddfc699"
   },
   "outputs": [],
   "source": [
    "#Reset column names for training set\n",
    "df.columns = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes',\n",
    "'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot',\n",
    "'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell',\n",
    "'su_attempted', 'num_root', 'num_file_creations', 'num_shells',\n",
    "'num_access_files', 'num_outbound_cmds', 'is_host_login',\n",
    "'is_guest_login', 'count', 'srv_count', 'serror_rate',\n",
    "'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate',\n",
    "'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n",
    "'dst_host_srv_count', 'dst_host_same_srv_rate','dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
    "'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n",
    "'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
    "'dst_host_srv_rerror_rate', 'subclass', 'difficulty_level']\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "LdxJa_6DWpzJ",
    "outputId": "e4e55423-e297-4c86-abc2-5b6592e5c919"
   },
   "outputs": [],
   "source": [
    "#Reset column names for testing set\n",
    "qp.columns = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes',\n",
    "'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot',\n",
    "'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell',\n",
    "'su_attempted', 'num_root', 'num_file_creations', 'num_shells',\n",
    "'num_access_files', 'num_outbound_cmds', 'is_host_login',\n",
    "'is_guest_login', 'count', 'srv_count', 'serror_rate',\n",
    "'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate',\n",
    "'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n",
    "'dst_host_srv_count', 'dst_host_same_srv_rate','dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
    "'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n",
    "'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
    "'dst_host_srv_rerror_rate', 'subclass', 'difficulty_level']\n",
    "qp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rw0BbZwBWpzL",
    "outputId": "e6b5ad2f-89c1-4e62-9fb0-64512064a3aa"
   },
   "outputs": [],
   "source": [
    "#accessing names of training columns\n",
    "lst_names = df.columns # returns a list of column names\n",
    "lst_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S4rV9Q61WpzN",
    "outputId": "4f6676e9-d073-451a-fcc4-97a5f122a3fc"
   },
   "outputs": [],
   "source": [
    "#accessing names of testing columns\n",
    "testlst_names = qp.columns\n",
    "testlst_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Dpf-0YRWpzQ",
    "outputId": "35fd45c0-38a4-4d9f-abb7-652505fc8a9d"
   },
   "outputs": [],
   "source": [
    "#Dropping the last columns of training set\n",
    "df = df.drop('difficulty_level', 1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dW5QL_0tWpzR",
    "outputId": "d9d920f0-c6fe-4cc1-fb83-924c193688f4"
   },
   "outputs": [],
   "source": [
    "#Dropping the last columns of testing set\n",
    "qp = qp.drop('difficulty_level', 1)\n",
    "qp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CNGpJU_oWpzV",
    "outputId": "122b6a53-c680-4d5b-bb23-35de591203c6"
   },
   "outputs": [],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0dcvaMKiWpzX",
    "outputId": "8291bb47-d5d5-4f1f-d68f-290711423fd5"
   },
   "outputs": [],
   "source": [
    "qp.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JveJ9rssWpzY",
    "outputId": "832c9b0e-55cf-4dae-c118-4622b00588f6"
   },
   "outputs": [],
   "source": [
    "#defining col list\n",
    "cols = ['protocol_type','service','flag']\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rYsBsG1XWpza"
   },
   "outputs": [],
   "source": [
    "#One-hot encoding\n",
    "def one_hot(df, cols):\n",
    "    \"\"\"\n",
    "    @param df pandas DataFrame\n",
    "    @param cols a list of columns to encode\n",
    "    @return a DataFrame with one-hot encoding\n",
    "    \"\"\"\n",
    "    for each in cols:\n",
    "        dummies = pd.get_dummies(df[each], prefix=each, drop_first=False)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "        df = df.drop(each, 1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ioHr_bvCWpzc"
   },
   "outputs": [],
   "source": [
    "#Merging train and test data\n",
    "combined_data = pd.concat([df,qp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rx6wwfW5Wpze",
    "outputId": "56da3877-b278-4545-da01-2f84630393cc"
   },
   "outputs": [],
   "source": [
    "#Applying one hot encoding to combined data\n",
    "combined_data = one_hot(combined_data,cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I3kfoXbtWpzg"
   },
   "outputs": [],
   "source": [
    "#Function to min-max normalize\n",
    "def normalize(df, cols):\n",
    "    \"\"\"\n",
    "    @param df pandas DataFrame\n",
    "    @param cols a list of columns to encode\n",
    "    @return a DataFrame with normalized specified features\n",
    "    \"\"\"\n",
    "    result = df.copy() # do not touch the original df\n",
    "    for feature_name in cols:\n",
    "        max_value = df[feature_name].max()\n",
    "        min_value = df[feature_name].min()\n",
    "        if max_value > min_value:\n",
    "            result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X3rVPR9UWpzh"
   },
   "outputs": [],
   "source": [
    "#Dropping subclass column for training set\n",
    "tmp = combined_data.pop('subclass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "NO1RBxcQWpzk",
    "outputId": "9b1407ca-9d32-43bc-a5f4-cdde0772e53f"
   },
   "outputs": [],
   "source": [
    "#Normalizing training set\n",
    "new_train_df = normalize(combined_data,combined_data.columns)\n",
    "new_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8COqwUUWpzm"
   },
   "outputs": [],
   "source": [
    "#Fixing labels for training set\n",
    "classlist = []\n",
    "check1 = (\"apache2\",\"back\",\"land\",\"neptune\",\"mailbomb\",\"pod\",\"processtable\",\"smurf\",\"teardrop\",\"udpstorm\",\"worm\")\n",
    "check2 = (\"ipsweep\",\"mscan\",\"nmap\",\"portsweep\",\"saint\",\"satan\")\n",
    "check3 = (\"buffer_overflow\",\"loadmodule\",\"perl\",\"ps\",\"rootkit\",\"sqlattack\",\"xterm\")\n",
    "check4 = (\"ftp_write\",\"guess_passwd\",\"httptunnel\",\"imap\",\"multihop\",\"named\",\"phf\",\"sendmail\",\"Snmpgetattack\",\"spy\",\"snmpguess\",\"warezclient\",\"warezmaster\",\"xlock\",\"xsnoop\")\n",
    "\n",
    "DoSCount=0\n",
    "ProbeCount=0\n",
    "U2RCount=0\n",
    "R2LCount=0\n",
    "NormalCount=0\n",
    "\n",
    "for item in tmp:\n",
    "    if item in check1:\n",
    "        classlist.append(\"DoS\")\n",
    "        DoSCount=DoSCount+1\n",
    "    elif item in check2:\n",
    "        classlist.append(\"Probe\")\n",
    "        ProbeCount=ProbeCount+1\n",
    "    elif item in check3:\n",
    "        classlist.append(\"U2R\")\n",
    "        U2RCount=U2RCount+1\n",
    "    elif item in check4:\n",
    "        classlist.append(\"R2L\")\n",
    "        R2LCount=R2LCount+1\n",
    "    else:\n",
    "        classlist.append(\"Normal\")\n",
    "        NormalCount=NormalCount+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "no8_kzghWpzn",
    "outputId": "413f287b-2cba-434a-c2f1-b66736a3ba3b"
   },
   "outputs": [],
   "source": [
    "DoSCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "id": "NEtt8jz1Wpzp",
    "outputId": "c041a1ae-cdad-41b7-eb3b-ae110c465a6f"
   },
   "outputs": [],
   "source": [
    "#Appending class column to training set\n",
    "new_train_df[\"Class\"] = classlist\n",
    "new_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "svZwVZDHWpzr",
    "outputId": "c0d5a47b-d942-4a19-a74a-7f6efa28c99b"
   },
   "outputs": [],
   "source": [
    "new_train_df[\"Class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pvz6KHEyWpzs",
    "outputId": "f808f9b5-6bdf-4754-ffd9-c8713689fe6c"
   },
   "outputs": [],
   "source": [
    "new_train_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YI49Yng0Wpzu",
    "outputId": "7cace233-8dec-4afb-cc76-2a310ba7c1e5"
   },
   "outputs": [],
   "source": [
    "y_train=new_train_df[\"Class\"]\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l-vU1GmxWpzw",
    "outputId": "627d91f3-7fc0-41aa-ea63-0a7f4cfdacb9"
   },
   "outputs": [],
   "source": [
    "y_train.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "id": "FFnhzcC7Wpzy",
    "outputId": "27e93780-0ae3-46e7-a616-2f0c16a8c80d"
   },
   "outputs": [],
   "source": [
    "combined_data_X = new_train_df.drop('Class', 1)\n",
    "combined_data_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur3aCjniWpz0"
   },
   "outputs": [],
   "source": [
    "oos_pred = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iz9CFMWjWpz2"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P5uICrRBWpz3",
    "outputId": "e145feb6-3f65-4252-857f-5cd2b1842708"
   },
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=2,shuffle=True,random_state=42)\n",
    "kfold.get_n_splits(combined_data_X,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Hp71RjnVFgf",
    "outputId": "fb62988a-a67d-413b-9d6c-add7f9db5e61"
   },
   "outputs": [],
   "source": [
    "#CNN\n",
    "batch_size = 128\n",
    "model = Sequential()\n",
    "model.add(Convolution1D(64, kernel_size=122, border_mode=\"same\",activation=\"relu\",input_shape=(122, 1)))\n",
    "model.add(MaxPooling1D(pool_length=(5)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5, activation=\"softmax\"))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BK-nHydFVuKf",
    "outputId": "580741f3-0287-44e9-e726-b5ae0b068ae6"
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZNkV7x5qVuvF",
    "outputId": "fa419000-dd48-475d-987c-159aecf75fb9"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "N6AEQDh9Vwv8",
    "outputId": "dbcf95b0-4772-465e-d73c-ffad74ffada0"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "for train_index, test_index in kfold.split(combined_data_X,y_train):\n",
    "    train_X, test_X = combined_data_X.iloc[train_index], combined_data_X.iloc[test_index]\n",
    "    train_y, test_y = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    print(\"train index:\",train_index)\n",
    "    print(\"test index:\",test_index)\n",
    "\n",
    "    x_columns_train = new_train_df.columns.drop('Class')\n",
    "    x_train_array = train_X[x_columns_train].values\n",
    "    x_train_1=np.reshape(x_train_array, (x_train_array.shape[0], x_train_array.shape[1], 1))\n",
    "\n",
    "    dummies = pd.get_dummies(train_y) # Classification\n",
    "    outcomes = dummies.columns\n",
    "    num_classes = len(outcomes)\n",
    "    y_train_1 = dummies.values\n",
    "\n",
    "    x_columns_test = new_train_df.columns.drop('Class')\n",
    "    x_test_array = test_X[x_columns_test].values\n",
    "    x_test_2=np.reshape(x_test_array, (x_test_array.shape[0], x_test_array.shape[1], 1))\n",
    "\n",
    "    dummies_test = pd.get_dummies(test_y) # Classification\n",
    "    outcomes_test = dummies_test.columns\n",
    "    num_classes = len(outcomes_test)\n",
    "    y_test_2 = dummies_test.values\n",
    "\n",
    "\n",
    "    history = model.fit(x_train_1, y_train_1,validation_data=(x_test_2,y_test_2), epochs=2)\n",
    "\n",
    "    pred = model.predict(x_test_2)\n",
    "    pred = np.argmax(pred,axis=1)\n",
    "    y_eval = np.argmax(y_test_2,axis=1)\n",
    "    score = metrics.accuracy_score(y_eval, pred)\n",
    "    oos_pred.append(score)\n",
    "    print(\"Validation score: {}\".format(score))\n",
    "\n",
    "\n",
    "    cnf_matrix =confusion_matrix(y_eval, pred)\n",
    "    FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)\n",
    "    FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "    TP = np.diag(cnf_matrix)\n",
    "    TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "    specificity = TP/(TP+FN)\n",
    "    sensitive = TN/(TN+FP)\n",
    "    print(\"CNN:Specificity:\" + str(specificity[0]))\n",
    "    print(\"CNN:Sensitivity:\" + str(sensitive[0]))\n",
    "\n",
    "\n",
    "    #confusion Matrix\n",
    "    matrix =confusion_matrix(y_eval, pred)\n",
    "    class_names=['DoS','Normal','Probe','R2L','U2R']\n",
    "    fig, ax = plt.subplots()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    sns.heatmap(pd.DataFrame(matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "    ax.xaxis.set_label_position(\"top\")\n",
    "    plt.tight_layout()\n",
    "    plt.title('Confusion matrix', y=1.1)\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "    target_names = ['DoS','Normal','Probe','R2L','U2R']\n",
    "    print(classification_report(y_eval, pred, target_names=target_names))\n",
    "\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    #Train and validation accuracy\n",
    "    plt.plot(epochs, acc, 'y', label='Training accurarcy')\n",
    "    plt.plot(epochs, val_acc, 'g', label='Validation accurarcy')\n",
    "    plt.title('Training and Validation accurarcy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "    #Train and validation loss\n",
    "    plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'g', label='Validation loss')\n",
    "    plt.title('Training and Validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1V3qzBzyWpz5",
    "outputId": "565ea2c2-5b63-4d21-917c-c67de8386c6f"
   },
   "outputs": [],
   "source": [
    "#CNN+LSTM\n",
    "batch_size = 128\n",
    "model = Sequential()\n",
    "model.add(Convolution1D(64, kernel_size=122, border_mode=\"same\",activation=\"relu\",input_shape=(122, 1)))\n",
    "model.add(MaxPooling1D(pool_length=(5)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
    "model.add(Reshape((128, 1), input_shape = (128, )))\n",
    "\n",
    "model.add(MaxPooling1D(pool_length=(5)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=False)))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pN8ISLwLWpz7",
    "outputId": "2c48c4e1-aa83-49de-ffaa-83e2b67f6fa5"
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81nEN2CkWpz8",
    "outputId": "b96b5bcf-3efb-4a44-98cf-236964db1a0e"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JXzZg-TrWpz_",
    "outputId": "ae004096-6df1-438e-9115-68d3bc6353c6"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "for train_index, test_index in kfold.split(combined_data_X,y_train):\n",
    "    train_X, test_X = combined_data_X.iloc[train_index], combined_data_X.iloc[test_index]\n",
    "    train_y, test_y = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    print(\"train index:\",train_index)\n",
    "    print(\"test index:\",test_index)\n",
    "\n",
    "    x_columns_train = new_train_df.columns.drop('Class')\n",
    "    x_train_array = train_X[x_columns_train].values\n",
    "    x_train_1=np.reshape(x_train_array, (x_train_array.shape[0], x_train_array.shape[1], 1))\n",
    "\n",
    "    dummies = pd.get_dummies(train_y) # Classification\n",
    "    outcomes = dummies.columns\n",
    "    num_classes = len(outcomes)\n",
    "    y_train_1 = dummies.values\n",
    "\n",
    "    x_columns_test = new_train_df.columns.drop('Class')\n",
    "    x_test_array = test_X[x_columns_test].values\n",
    "    x_test_2=np.reshape(x_test_array, (x_test_array.shape[0], x_test_array.shape[1], 1))\n",
    "\n",
    "    dummies_test = pd.get_dummies(test_y) # Classification\n",
    "    outcomes_test = dummies_test.columns\n",
    "    num_classes = len(outcomes_test)\n",
    "    y_test_2 = dummies_test.values\n",
    "\n",
    "\n",
    "    history = model.fit(x_train_1, y_train_1,validation_data=(x_test_2,y_test_2), epochs=2)\n",
    "\n",
    "    pred = model.predict(x_test_2)\n",
    "    pred = np.argmax(pred,axis=1)\n",
    "    y_eval = np.argmax(y_test_2,axis=1)\n",
    "    score = metrics.accuracy_score(y_eval, pred)\n",
    "    oos_pred.append(score)\n",
    "    print(\"Validation score: {}\".format(score))\n",
    "\n",
    "\n",
    "    cnf_matrix =confusion_matrix(y_eval, pred)\n",
    "    FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)\n",
    "    FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "    TP = np.diag(cnf_matrix)\n",
    "    TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "    specificity = TP/(TP+FN)\n",
    "    sensitive = TN/(TN+FP)\n",
    "    print(\"CNN+LSTM:Specificity:\" + str(specificity[0]))\n",
    "    print(\"CNN+LSTM:Sensitivity:\" + str(sensitive[0]))\n",
    "\n",
    "\n",
    "    #confusion Matrix\n",
    "    matrix =confusion_matrix(y_eval, pred)\n",
    "    class_names=['DoS','Normal','Probe','R2L','U2R']\n",
    "    fig, ax = plt.subplots()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    sns.heatmap(pd.DataFrame(matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "    ax.xaxis.set_label_position(\"top\")\n",
    "    plt.tight_layout()\n",
    "    plt.title('Confusion matrix', y=1.1)\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "    target_names = ['DoS','Normal','Probe','R2L','U2R']\n",
    "    print(classification_report(y_eval, pred, target_names=target_names))\n",
    "\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    #Train and validation accuracy\n",
    "    plt.plot(epochs, acc, 'y', label='Training accurarcy')\n",
    "    plt.plot(epochs, val_acc, 'g', label='Validation accurarcy')\n",
    "    plt.title('Training and Validation accurarcy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "    #Train and validation loss\n",
    "    plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'g', label='Validation loss')\n",
    "    plt.title('Training and Validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bRxtocfTRvGh"
   },
   "outputs": [],
   "source": [
    "#RNN-LSTM\n",
    "batch_size = 128\n",
    "model = Sequential()\n",
    "model.add(keras.layers.LSTM(64,input_shape=(122, 1),activation='tanh',recurrent_activation='hard_sigmoid'))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T8yCyZEpxYhi",
    "outputId": "b1473b2e-2701-4b4c-9dc3-dfb231b19498"
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LA026KEJYeAY",
    "outputId": "52064c6d-30e0-43e9-b6d3-d407600adcec"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "KFehJJwFxiDo",
    "outputId": "d2465b83-6d81-4eb4-afae-95045f683a30"
   },
   "outputs": [],
   "source": [
    "for train_index, test_index in kfold.split(combined_data_X,y_train):\n",
    "    train_X, test_X = combined_data_X.iloc[train_index], combined_data_X.iloc[test_index]\n",
    "    train_y, test_y = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    print(\"train index:\",train_index)\n",
    "    print(\"test index:\",test_index)\n",
    "\n",
    "    x_columns_train = new_train_df.columns.drop('Class')\n",
    "    x_train_array = train_X[x_columns_train].values\n",
    "    x_train_1=np.reshape(x_train_array, (x_train_array.shape[0], x_train_array.shape[1], 1))\n",
    "\n",
    "    dummies = pd.get_dummies(train_y) # Classification\n",
    "    outcomes = dummies.columns\n",
    "    num_classes = len(outcomes)\n",
    "    y_train_1 = dummies.values\n",
    "\n",
    "    x_columns_test = new_train_df.columns.drop('Class')\n",
    "    x_test_array = test_X[x_columns_test].values\n",
    "    x_test_2=np.reshape(x_test_array, (x_test_array.shape[0], x_test_array.shape[1], 1))\n",
    "\n",
    "    dummies_test = pd.get_dummies(test_y) # Classification\n",
    "    outcomes_test = dummies_test.columns\n",
    "    num_classes = len(outcomes_test)\n",
    "    y_test_2 = dummies_test.values\n",
    "\n",
    "\n",
    "    history = model.fit(x_train_1, y_train_1,validation_data=(x_test_2,y_test_2), epochs=2)\n",
    "\n",
    "    pred = model.predict(x_test_2)\n",
    "    pred = np.argmax(pred,axis=1)\n",
    "    y_eval = np.argmax(y_test_2,axis=1)\n",
    "    score = metrics.accuracy_score(y_eval, pred)\n",
    "    oos_pred.append(score)\n",
    "    print(\"Validation score: {}\".format(score))\n",
    "\n",
    "\n",
    "    cnf_matrix =confusion_matrix(y_eval, pred)\n",
    "    FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)\n",
    "    FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "    TP = np.diag(cnf_matrix)\n",
    "    TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "    specificity = TP/(TP+FN)\n",
    "    sensitive = TN/(TN+FP)\n",
    "    print(\"RNN:Specificity:\" + str(specificity[0]))\n",
    "    print(\"RNN:Sensitivity:\" + str(sensitive[0]))\n",
    "\n",
    "\n",
    "    #confusion Matrix\n",
    "    matrix =confusion_matrix(y_eval, pred)\n",
    "    class_names=['DoS','Normal','Probe','R2L','U2R']\n",
    "    fig, ax = plt.subplots()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    sns.heatmap(pd.DataFrame(matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "    ax.xaxis.set_label_position(\"top\")\n",
    "    plt.tight_layout()\n",
    "    plt.title('Confusion matrix', y=1.1)\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "    target_names = ['DoS','Normal','Probe','R2L','U2R']\n",
    "    print(classification_report(y_eval, pred, target_names=target_names))\n",
    "\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    #Train and validation accuracy\n",
    "    plt.plot(epochs, acc, 'y', label='Training accurarcy')\n",
    "    plt.plot(epochs, val_acc, 'g', label='Validation accurarcy')\n",
    "    plt.title('Training and Validation accurarcy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "    #Train and validation loss\n",
    "    plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'g', label='Validation loss')\n",
    "    plt.title('Training and Validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
